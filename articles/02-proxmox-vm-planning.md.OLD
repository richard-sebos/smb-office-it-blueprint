---
title: "Proxmox VM Planning and Best Practices for SMB Infrastructure"
description: "Learn how to properly plan, size, and configure virtual machines in Proxmox for a production-ready small business IT environment with 11 VMs running Active Directory, file services, and workstations."
---

# Proxmox VM Planning and Best Practices for SMB Infrastructure

## Why Proper VM Planning Matters

You can't just throw virtual machines at Proxmox and hope for the best. I've seen too many environments where someone said "just give everything 4GB of RAM" and then wondered why their file server crawled or their domain controller randomly locked up during user authentication.

**Here's the reality:** A poorly planned virtualization environment will perform worse than bare metal servers, cost more in hardware, and create operational headaches that make you question why you virtualized in the first place.

But when done right, virtualization on Proxmox gives you:

- **Resource efficiency:** Run 11+ servers on hardware that couldn't support 3 bare metal systems
- **Flexibility:** Migrate VMs, create snapshots, test changes without risk
- **Cost savings:** One good server instead of a rack full of hardware
- **Management simplicity:** Single interface for your entire infrastructure

This article walks through the **exact planning process** for our 11-VM SMB infrastructure, with resource calculations, storage decisions, and network design that you can adapt to your own environment.

## The 11-VM Architecture Overview

Our SMB environment includes:

### Infrastructure Services (Critical - Must Stay Running)
1. **DC01** - Primary Samba AD Domain Controller
2. **DC02** - Secondary Domain Controller (redundancy)
3. **FILES01** - File server with departmental shares
4. **PRINT01** - Print server (CUPS)

### Workstation VMs (User-Facing)
5. **ADMIN-WS01** - Admin Assistant workstation
6. **HR-WS01** - HR Manager workstation
7. **FIN-WS01** - Finance Manager workstation
8. **EXEC-WS01** - Executive/Managing Partner workstation
9. **PROJ-WS01** - Project/Consulting professional workstation
10. **INTERN-WS01** - Intern/temporary worker workstation

### Management/Monitoring
11. **MGMT01** - Ansible control node, monitoring, backups

**Why 11 VMs?** This represents a realistic small business with:
- Department separation (HR, Finance, Operations)
- Role variety (executives, managers, assistants, interns)
- Redundancy where it matters (dual domain controllers)
- Real-world complexity without being overwhelming

## Hardware Requirements: What You Actually Need

Let's start with reality: You don't need a $10,000 server to run this lab.

### Minimum Viable Hardware

**For Learning/Lab Environment:**
- **CPU:** 6-core (12 threads) - Intel i5/i7 or AMD Ryzen 5/7
- **RAM:** 32GB DDR4
- **Storage:** 500GB NVMe SSD (or 1TB SATA SSD)
- **Network:** Gigabit NIC (onboard is fine)

**Cost:** ~$800-1200 used enterprise server or custom build

**For Production/Heavier Workloads:**
- **CPU:** 8-core (16 threads) - Intel Xeon or AMD EPYC/Threadripper
- **RAM:** 64GB DDR4 ECC
- **Storage:** 1TB NVMe SSD + 2TB SATA for backups
- **Network:** Dual gigabit NICs (for redundancy/VLANs)

**Cost:** ~$1500-2500 for used enterprise or custom build

### Why These Specs?

**CPU:** Modern virtualization relies on CPU overcommitment. With 6 cores (12 threads), you can assign 2 vCPUs to each of 11 VMs because most VMs aren't CPU-intensive simultaneously. Domain controllers and file servers spend most of their time waiting for I/O, not computing.

**RAM:** This is your constraint. RAM cannot be overcommitted safely. Our 11 VMs need:
- 2x Domain Controllers: 2GB each = 4GB
- 1x File Server: 4GB
- 1x Print Server: 2GB
- 6x Workstations: 2-3GB each = 15GB
- 1x Management: 2GB
- Proxmox host: 4GB

**Total: ~31GB** (32GB works, 64GB gives headroom)

**Storage:** Modern SSDs make this viable. You'll store:
- VM disks: ~200-300GB (thin provisioned)
- Templates: ~20GB
- ISO images: ~10GB
- Backups: ~150GB (using thin snapshots)
- Proxmox system: ~20GB

500GB is minimum, 1TB is comfortable.

## VM Resource Allocation Strategy

Here's the detailed breakdown for our 11-VM environment:

### Domain Controllers (DC01, DC02)

**Purpose:** Samba AD authentication, DNS, Kerberos

| Resource | Allocation | Reasoning |
|----------|------------|-----------|
| vCPU | 2 cores | AD is single-threaded for most operations; 2 cores handle concurrent auth requests |
| RAM | 2GB | Samba AD + DNS + Kerberos; generous for <100 users |
| Disk | 32GB | OS (20GB) + AD database (5GB) + logs (5GB) + growth |
| Network | 1 NIC (virtio) | Dedicated bridge, low bandwidth needs |

**Best Practices:**
- Use virtio drivers for network and disk (much faster than emulated)
- Enable CPU "host" type for better performance
- Place both DCs on different Proxmox storage pools if possible (redundancy)
- **DO NOT** overcommit CPU or RAM on domain controllers - they need guaranteed resources

**Storage Type:** Local SSD (fastest)

### File Server (FILES01)

**Purpose:** Departmental shares (HR, Finance, Projects, Shared)

| Resource | Allocation | Reasoning |
|----------|------------|-----------|
| vCPU | 2-4 cores | Samba file serving benefits from multiple cores for concurrent connections |
| RAM | 4GB | 2GB for OS, 2GB for Samba + file caching |
| Disk | 100GB | OS (20GB) + shares (60GB) + growth (20GB) |
| Network | 1 NIC (virtio) | Can add second NIC for isolated backup network |

**Best Practices:**
- Allocate extra disk via second virtual disk if shares grow
- Enable "discard" on VM disk to reclaim space from deleted files
- Use qcow2 format with thin provisioning
- Monitor I/O - file servers are I/O intensive
- Consider passing through a dedicated disk controller if you have heavy file I/O

**Storage Type:** Local SSD preferred, SATA acceptable

### Print Server (PRINT01)

**Purpose:** CUPS print queue management

| Resource | Allocation | Reasoning |
|----------|------------|-----------|
| vCPU | 1-2 cores | Print spooling is lightweight |
| RAM | 2GB | CUPS + print job spooling |
| Disk | 32GB | OS (20GB) + spool space (10GB) |
| Network | 1 NIC (virtio) | Low bandwidth |

**Best Practices:**
- Minimal resources - print servers don't need much
- Can be combined with file server if resources are tight
- Set disk cache mode to "writethrough" for print queue integrity

**Storage Type:** Any (even spinning disk acceptable)

### Workstations (Various)

**Purpose:** Linux desktop VMs for different user roles

| Resource | Standard | Power User | Reasoning |
|----------|----------|------------|-----------|
| vCPU | 2 cores | 4 cores | Desktop needs: 2 sufficient, 4 for responsiveness |
| RAM | 2-3GB | 4GB | Gnome/KDE desktop + apps; executives get more |
| Disk | 32GB | 40GB | OS (20GB) + apps (5GB) + user data (minimal, on file shares) |
| Network | 1 NIC | 1 NIC | Standard throughput |

**Role-Specific Allocations:**
- **ADMIN-WS01:** 2 vCPU, 2GB RAM (basic office tasks)
- **HR-WS01:** 2 vCPU, 3GB RAM (HR software, documents)
- **FIN-WS01:** 2 vCPU, 4GB RAM (accounting software, spreadsheets)
- **EXEC-WS01:** 4 vCPU, 4GB RAM (smooth experience, multiple apps)
- **PROJ-WS01:** 2 vCPU, 3GB RAM (project tools, documentation)
- **INTERN-WS01:** 2 vCPU, 2GB RAM (minimal, temporary)

**Best Practices:**
- Enable VirtIO-GPU for better graphics performance
- Use SPICE or VNC for remote console (not for daily use - users should RDP/SSH)
- Workstations should authenticate via SSSD, store data on file server
- Use thin provisioning - actual disk usage will be much less than allocated

**Storage Type:** Local SSD recommended for desktop responsiveness

### Management Node (MGMT01)

**Purpose:** Ansible control node, monitoring, scripts

| Resource | Allocation | Reasoning |
|----------|------------|-----------|
| vCPU | 2 cores | Script execution, Ansible playbooks |
| RAM | 2-4GB | Depends on monitoring stack |
| Disk | 40GB | OS (20GB) + Ansible roles (5GB) + logs/backups (15GB) |
| Network | 1 NIC | Management traffic only |

**Best Practices:**
- SSH access to all other VMs
- Store Ansible inventory, playbooks, scripts
- Can run monitoring (Prometheus, Grafana) if RAM increased to 4-6GB
- Keep backed up - this VM contains your automation

**Storage Type:** Local SSD

## Complete Resource Matrix

Here's the full allocation table:

| VM Name | vCPU | RAM (GB) | Disk (GB) | Purpose | Priority |
|---------|------|----------|-----------|---------|----------|
| DC01 | 2 | 2 | 32 | Primary Domain Controller | **CRITICAL** |
| DC02 | 2 | 2 | 32 | Secondary Domain Controller | **CRITICAL** |
| FILES01 | 4 | 4 | 100 | File Server | **HIGH** |
| PRINT01 | 2 | 2 | 32 | Print Server | MEDIUM |
| ADMIN-WS01 | 2 | 2 | 32 | Admin Assistant Workstation | MEDIUM |
| HR-WS01 | 2 | 3 | 32 | HR Manager Workstation | MEDIUM |
| FIN-WS01 | 2 | 4 | 32 | Finance Manager Workstation | HIGH |
| EXEC-WS01 | 4 | 4 | 40 | Executive Workstation | HIGH |
| PROJ-WS01 | 2 | 3 | 32 | Project Professional Workstation | MEDIUM |
| INTERN-WS01 | 2 | 2 | 32 | Intern Workstation | LOW |
| MGMT01 | 2 | 3 | 40 | Management/Ansible Node | HIGH |
| **TOTAL** | **26 vCPU** | **31 GB** | **436 GB** | | |

**With Proxmox Host Overhead:** ~32GB RAM, ~500GB disk, 6-8 physical cores

## Storage Strategy: Where to Put What

Proxmox supports multiple storage types. Here's how to use them effectively:

### Storage Types Explained

**Local (Proxmox Host Disk):**
- Best for: VM disks, templates, ISOs
- Performance: Excellent (if SSD)
- Redundancy: None (single host)
- Format: Directory, LVM, LVM-thin, ZFS

**Network Storage (NFS, iSCSI, Ceph):**
- Best for: Shared storage, live migration
- Performance: Good (depends on network)
- Redundancy: Yes (if configured)
- Format: Various

**For This Project:** We'll use **local SSD storage** with **LVM-thin** provisioning.

### Why LVM-Thin?

**Thin Provisioning** means you allocate 100GB to a VM, but it only uses disk space for data actually written. Benefits:

- Allocate 436GB across VMs, actually use ~200GB
- Snapshots are fast and efficient
- Easy to extend volumes when needed

**Alternative:** ZFS gives you compression, snapshots, and better data integrity, but requires more RAM and CPU overhead. For a learning lab, LVM-thin is simpler.

### Storage Layout

```
/dev/sda (500GB NVMe SSD)
├── 100GB - Proxmox root filesystem (ext4)
├── 400GB - VM storage pool (LVM-thin)
    └── VMs stored here with thin provisioning
```

**Configuration in Proxmox:**
1. Datacenter → Storage
2. Create LVM-Thin volume group
3. Set as default for VM disks
4. Enable thin provisioning

### Disk Performance Tuning

**For each VM, set these disk options:**

| Setting | Value | Why |
|---------|-------|-----|
| Cache | Write-through | Prevents data corruption on host crashes |
| Discard | Enabled | Allows TRIM/discard for thin provisioning |
| IO Thread | Enabled | Offloads I/O to dedicated thread |
| SSD Emulation | Enabled | Guest OS can use TRIM |

**For domain controllers and file server, use:**
- Cache: None (direct I/O, better for databases)
- AIO: Native (better performance)

## Network Architecture: VLANs and Bridges

Proper network segmentation is critical for security and performance.

### Network Design

We'll use **three network segments:**

1. **Management Network (VLAN 10):** Proxmox host, SSH access, Ansible
   - Subnet: 10.0.10.0/24
   - Gateway: 10.0.10.1

2. **Server Network (VLAN 20):** Domain controllers, file/print servers
   - Subnet: 10.0.20.0/24
   - Gateway: 10.0.20.1

3. **Workstation Network (VLAN 30):** User workstations
   - Subnet: 10.0.30.0/24
   - Gateway: 10.0.30.1

**Why Separate Networks?**
- **Security:** Segment workstations from infrastructure
- **Traffic Control:** Prevent broadcast storms
- **Troubleshooting:** Isolate network issues
- **Compliance:** Separate financial/HR systems from general workstations

### Proxmox Network Bridge Configuration

**Create Linux Bridges in Proxmox:**

```bash
# Management Bridge (vmbr0)
auto vmbr0
iface vmbr0 inet static
    address 10.0.10.10/24
    gateway 10.0.10.1
    bridge-ports eno1
    bridge-stp off
    bridge-fd 0

# Server VLAN Bridge (vmbr1)
auto vmbr1
iface vmbr1 inet manual
    bridge-ports eno1.20
    bridge-stp off
    bridge-fd 0
    bridge-vlan-aware yes

# Workstation VLAN Bridge (vmbr2)
iface vmbr2 inet manual
    bridge-ports eno1.30
    bridge-stp off
    bridge-fd 0
    bridge-vlan-aware yes
```

**VM Network Assignments:**

| VM | Bridge | VLAN | IP Address |
|----|--------|------|------------|
| DC01 | vmbr1 | 20 | 10.0.20.11 |
| DC02 | vmbr1 | 20 | 10.0.20.12 |
| FILES01 | vmbr1 | 20 | 10.0.20.21 |
| PRINT01 | vmbr1 | 20 | 10.0.20.22 |
| ADMIN-WS01 | vmbr2 | 30 | 10.0.30.31 |
| HR-WS01 | vmbr2 | 30 | 10.0.30.32 |
| FIN-WS01 | vmbr2 | 30 | 10.0.30.33 |
| EXEC-WS01 | vmbr2 | 30 | 10.0.30.34 |
| PROJ-WS01 | vmbr2 | 30 | 10.0.30.35 |
| INTERN-WS01 | vmbr2 | 30 | 10.0.30.36 |
| MGMT01 | vmbr0 | 10 | 10.0.10.20 |

**DNS Configuration:**
- Primary DNS: 10.0.20.11 (DC01)
- Secondary DNS: 10.0.20.12 (DC02)
- All VMs point to DCs for name resolution

## VM Template Strategy

Don't create VMs from scratch every time. Build **templates** once, clone forever.

### Creating Base Templates

**Template 1: Oracle Linux 9 (Server)**

1. Install minimal Oracle Linux 9
2. Update all packages: `dnf update -y`
3. Install base tools: `dnf install -y vim wget curl net-tools`
4. Configure cloud-init (for Proxmox customization)
5. Remove SSH host keys: `rm /etc/ssh/ssh_host_*`
6. Clear machine ID: `truncate -s 0 /etc/machine-id`
7. Clean yum cache: `dnf clean all`
8. Power down
9. Convert to template in Proxmox

**Use for:** DC01, DC02, FILES01, PRINT01, MGMT01

**Template 2: Ubuntu Server 22.04 LTS**

1. Install minimal Ubuntu Server
2. Update: `apt update && apt upgrade -y`
3. Install base tools: `apt install -y vim wget curl net-tools`
4. Install cloud-init: `apt install -y cloud-init`
5. Clean cloud-init: `cloud-init clean`
6. Remove SSH host keys: `rm /etc/ssh/ssh_host_*`
7. Clear machine ID: `truncate -s 0 /etc/machine-id`
8. Clean apt cache: `apt clean`
9. Power down
10. Convert to template

**Use for:** Workstation VMs (with desktop environment added post-clone)

### Cloning VMs from Templates

**Proxmox GUI:**
1. Right-click template → Clone
2. Select "Full Clone" (linked clones have dependencies)
3. Set new VM ID and name
4. Customize resources (CPU, RAM, disk)

**Proxmox CLI (Faster for Multiple VMs):**

```bash
# Clone template 100 to create DC01 (VM ID 101)
qm clone 100 101 --name DC01 --full

# Resize disk if needed
qm resize 101 scsi0 +20G

# Set CPU and RAM
qm set 101 --cores 2 --memory 2048

# Set network
qm set 101 --net0 virtio,bridge=vmbr1,tag=20
```

**Ansible Automation (Coming in Later Article):**
We'll automate VM cloning, customization, and provisioning using Ansible's Proxmox modules.

## Performance Best Practices

### CPU Configuration

**CPU Type:** Set to "host" for best performance
- Exposes all host CPU features to guest
- Better than emulated CPU types (kvm64, qemu64)
- VMs can use hardware virtualization extensions

**CPU Units:** Weight CPU allocation
- Default: 1024
- Critical VMs (DC01, DC02): 2048 (2x priority)
- Workstations: 1024 (standard)
- Low priority (INTERN-WS01): 512

**NUMA:** Not needed for our host size
- Only matters with >2 CPU sockets
- Our single CPU doesn't benefit

### Memory Configuration

**Ballooning:** Disabled for production VMs
- Ballooning allows Proxmox to reclaim "unused" RAM
- Can cause performance issues if guest needs RAM suddenly
- **Disable for:** DC01, DC02, FILES01
- **Enable for:** Workstations (can tolerate brief delays)

**Minimum RAM:** Set minimum guaranteed RAM
- Ensures VM always has minimum resources
- Set to 75% of allocated RAM

**Example:**
- Allocated: 4GB
- Minimum: 3GB
- Allows Proxmox to balloon 1GB if host is under pressure

### Disk I/O

**I/O Threads:** Enable for all VMs
- Offloads disk I/O to dedicated threads
- Reduces CPU overhead
- Noticeable improvement on SSDs

**I/O Limits:** Set for fairness
- Prevents single VM from saturating disk
- Example limits (MB/s):
  - Domain Controllers: 200 MB/s
  - File Server: 300 MB/s
  - Workstations: 100 MB/s

**SSD Emulation:** Enable
- Allows guest to use TRIM/discard
- Keeps thin provisioning efficient
- Guest sees SSD, uses appropriate I/O scheduler

### Network Performance

**VirtIO Drivers:** Always use VirtIO
- Paravirtualized network driver
- 3-5x faster than emulated e1000
- Supported by all modern Linux distros

**Multi-Queue:** Enable for high-traffic VMs
- Allows network I/O across multiple CPU cores
- Set queues = number of vCPUs (up to 8)
- Enable on: FILES01, Domain Controllers

**Firewall:** Disable on Proxmox bridges (if using VM firewalls)
- Reduces overhead
- Configure firewalls inside VMs instead

## Backup Strategy

You will lose data. Plan for it.

### Proxmox Built-in Backup

**Backup Schedule:**
- **Critical VMs (DC01, DC02, FILES01):** Daily
- **Important VMs (Workstations, MGMT01):** Weekly
- **Low Priority (INTERN-WS01):** Monthly or none

**Backup Modes:**

| Mode | Downtime | Speed | Use Case |
|------|----------|-------|----------|
| Snapshot | None | Fast | Running VMs, LVM-thin storage |
| Suspend | Brief pause | Fast | VMs that can tolerate pause |
| Stop | Full shutdown | Fastest | VMs you can stop |

**Use Snapshot mode** for all VMs (requires LVM-thin or ZFS)

**Retention:**
- Daily backups: Keep 7 days
- Weekly backups: Keep 4 weeks
- Monthly backups: Keep 3 months

**Backup Storage:**
- **Best:** External NAS via NFS (separate from Proxmox host)
- **Acceptable:** Second local disk (not the same as VM storage)
- **Last Resort:** Same disk (better than nothing, but doesn't protect from disk failure)

### Backup Testing

**Monthly:** Restore one VM from backup to verify integrity
- Don't just assume backups work
- Restore to test VM ID, boot, verify

## High Availability Considerations

Our single-host setup isn't truly "highly available," but we can improve resilience:

### Redundancy Within Single Host

**Dual Domain Controllers:** If DC01 fails, DC02 takes over
- Users can still authenticate
- DNS still resolves
- No single point of failure for authentication

**Snapshot Before Changes:** Before major changes, snapshot VMs
- Quick rollback if something breaks
- Snapshots are cheap with LVM-thin

**UPS:** Invest in a good UPS
- Protects from power failures
- Gives time for graceful shutdown
- ~$200-400 for quality unit

### Future: Multi-Host Clustering

If you grow beyond one server:
- Add second Proxmox host
- Create Proxmox cluster
- Enable HA (VMs auto-migrate on host failure)
- Shared storage (Ceph, NFS) for live migration

**For now:** Document everything so you can rebuild quickly if hardware fails.

## Monitoring and Maintenance

### Built-in Proxmox Monitoring

**Watch These Metrics:**
- Host CPU utilization (should average <60%)
- Host RAM usage (should stay <80%)
- Disk I/O wait (should be <5%)
- Network throughput (should not saturate gigabit)

**Warning Signs:**
- CPU steal time >10% (VMs contending for CPU)
- RAM ballooning aggressive (host under-provisioned)
- High I/O wait (disk bottleneck)

### VM-Level Monitoring

**Install on MGMT01:**
- Prometheus (metrics collection)
- Grafana (visualization)
- Node Exporter on each VM (system metrics)

**Track:**
- VM CPU/RAM usage over time
- Disk space trends
- Authentication failures (from auditd)
- Samba connection counts

### Maintenance Windows

**Monthly Tasks:**
1. Apply OS updates to all VMs (Ansible playbook)
2. Review backup logs
3. Test one backup restore
4. Review disk usage, expand if needed
5. Check Proxmox updates

**Quarterly Tasks:**
1. Review VM resource allocations (right-sizing)
2. Clean old snapshots
3. Rotate backup retention
4. Security audit (covered in future article)

## Common Mistakes to Avoid

### Over-Provisioning Everything

**Mistake:** "I'll give every VM 8GB RAM and 4 CPUs just to be safe"

**Reality:** You run out of host resources and can't start VMs

**Solution:** Start conservative, monitor, adjust based on actual usage

### Under-Provisioning Critical Systems

**Mistake:** "Domain controller only needs 512MB RAM"

**Reality:** Authentication delays, random lockups, user complaints

**Solution:** Give infrastructure VMs (DCs, file servers) guaranteed resources

### No Network Segmentation

**Mistake:** All VMs on same flat network

**Reality:** Security issues, broadcast storms, difficult troubleshooting

**Solution:** Use VLANs and bridges to segment traffic

### Skipping Backups

**Mistake:** "I'll set up backups later"

**Reality:** Data loss after accidental deletion, corruption, or host failure

**Solution:** Configure backups on day 1, test them monthly

### Using Emulated Hardware

**Mistake:** Default to e1000 network, IDE disk

**Reality:** Terrible performance, wasted host resources

**Solution:** Always use VirtIO for network and disk

### No Documentation

**Mistake:** "I'll remember how I configured this"

**Reality:** Six months later, you have no idea what VM 117 is for

**Solution:** Document VM purposes, IP addresses, resource allocations (we'll cover this with Ansible inventory)

## Organizing Your Proxmox Environment

Managing 11+ VMs becomes chaotic without proper organization. Proxmox provides several powerful features to keep your environment structured and maintainable.

### Resource Pools: Grouping VMs Logically

**Resource Pools** let you group VMs for bulk operations, permissions, and resource tracking.

**Why Use Pools?**
- Perform operations on multiple VMs at once (start/stop/backup)
- Assign permissions by department (HR staff only see HR pool)
- Track resource usage by category
- Organize complex environments logically

**Our Pool Strategy:**

| Pool Name | VMs Included | Purpose |
|-----------|-------------|---------|
| `infrastructure` | DC01, DC02, FILES01, PRINT01 | Critical infrastructure services |
| `workstations-hr` | HR-WS01 | HR department workstations |
| `workstations-finance` | FIN-WS01 | Finance department workstations |
| `workstations-admin` | ADMIN-WS01, EXEC-WS01 | Administrative/executive workstations |
| `workstations-projects` | PROJ-WS01 | Project/consulting workstations |
| `workstations-temp` | INTERN-WS01 | Temporary/intern workstations |
| `management` | MGMT01 | Management and automation VMs |

**Creating Pools via GUI:**
1. Datacenter → Permissions → Pools
2. Click "Create"
3. Enter pool name
4. Add VMs to pool

**Creating Pools via CLI:**

```bash
# Create infrastructure pool
pvesh create /pools --poolid infrastructure --comment "Critical Infrastructure Services"

# Add VMs to pool
pvesh set /pools/infrastructure --vms 101,102,121,122

# Create workstation pools
pvesh create /pools --poolid workstations-hr --comment "HR Department Workstations"
pvesh create /pools --poolid workstations-finance --comment "Finance Department Workstations"
pvesh create /pools --poolid workstations-admin --comment "Admin/Executive Workstations"
pvesh create /pools --poolid workstations-projects --comment "Project Team Workstations"
pvesh create /pools --poolid workstations-temp --comment "Temporary/Intern Workstations"
pvesh create /pools --poolid management --comment "Management and Automation"

# Add VMs to respective pools
pvesh set /pools/workstations-hr --vms 132
pvesh set /pools/workstations-finance --vms 133
pvesh set /pools/workstations-admin --vms 131,134
pvesh set /pools/workstations-projects --vms 135
pvesh set /pools/workstations-temp --vms 136
pvesh set /pools/management --vms 140
```

**Pool Benefits:**

**Bulk Start/Stop:**
```bash
# Stop all workstations at night
for vm in $(pvesh get /pools/workstations-hr --vms); do
    qm stop $vm
done
```

**Backup by Pool:**
- Create backup job for "infrastructure" pool (daily)
- Create backup job for "workstations-*" pools (weekly)
- Automatically includes new VMs added to pool

**Permission Assignment:**
```bash
# Give HR manager access to only HR workstations pool
pveum acl modify /pool/workstations-hr --roles PVEVMUser --users hrmanager@pve
```

### Tags: Flexible Labeling System

**Tags** are a newer Proxmox feature (6.2+) that provide flexible, multi-category labeling.

**Why Use Tags?**
- VMs can have multiple tags (unlike pools)
- Quick visual identification in GUI
- Filter views by tag
- Organize across multiple dimensions

**Our Tagging Strategy:**

**By Function:**
- `domain-controller` - DC01, DC02
- `file-server` - FILES01
- `print-server` - PRINT01
- `workstation` - All workstation VMs
- `management` - MGMT01

**By Department:**
- `dept-hr` - HR-WS01, (HR has access to FILES01 shares)
- `dept-finance` - FIN-WS01
- `dept-admin` - ADMIN-WS01, EXEC-WS01
- `dept-projects` - PROJ-WS01

**By Priority:**
- `critical` - DC01, DC02 (must stay running)
- `high` - FILES01, FIN-WS01, EXEC-WS01, MGMT01
- `medium` - PRINT01, HR-WS01, ADMIN-WS01, PROJ-WS01
- `low` - INTERN-WS01

**By Operating System:**
- `oracle-linux-9` - Infrastructure VMs
- `ubuntu-22.04` - Workstation VMs

**By Backup Schedule:**
- `backup-daily` - Critical VMs
- `backup-weekly` - Standard VMs
- `backup-monthly` - Low priority VMs

**Adding Tags via GUI:**
1. Select VM
2. Click "Edit"
3. In "Tags" field, enter comma-separated tags: `domain-controller,critical,backup-daily,oracle-linux-9`
4. Tags appear as colored labels in VM list

**Adding Tags via CLI:**

```bash
# Tag DC01
qm set 101 --tags "domain-controller,critical,backup-daily,oracle-linux-9,infrastructure"

# Tag DC02
qm set 102 --tags "domain-controller,critical,backup-daily,oracle-linux-9,infrastructure"

# Tag FILES01
qm set 121 --tags "file-server,high,backup-daily,oracle-linux-9,infrastructure"

# Tag PRINT01
qm set 122 --tags "print-server,medium,backup-weekly,oracle-linux-9,infrastructure"

# Tag HR-WS01
qm set 132 --tags "workstation,dept-hr,medium,backup-weekly,ubuntu-22.04"

# Tag FIN-WS01
qm set 133 --tags "workstation,dept-finance,high,backup-weekly,ubuntu-22.04"

# Tag ADMIN-WS01
qm set 131 --tags "workstation,dept-admin,medium,backup-weekly,ubuntu-22.04"

# Tag EXEC-WS01
qm set 134 --tags "workstation,dept-admin,high,backup-weekly,ubuntu-22.04"

# Tag PROJ-WS01
qm set 135 --tags "workstation,dept-projects,medium,backup-weekly,ubuntu-22.04"

# Tag INTERN-WS01
qm set 136 --tags "workstation,dept-temp,low,backup-monthly,ubuntu-22.04"

# Tag MGMT01
qm set 140 --tags "management,high,backup-daily,oracle-linux-9"
```

**Using Tags:**

**Filter in GUI:** Click tag to show only VMs with that tag

**Filter in CLI:**
```bash
# List all critical VMs
qm list | grep critical

# Find all workstations
pvesh get /cluster/resources --type vm | jq '.[] | select(.tags | contains("workstation"))'
```

**Automated Operations by Tag:**
```bash
# Stop all low-priority VMs
for vmid in $(qm list | grep low | awk '{print $1}'); do
    qm stop $vmid
done
```

### VM Notes and Descriptions

**VM Notes** provide detailed documentation visible in Proxmox GUI.

**What to Document:**

**Standard Information:**
- VM purpose and business function
- Assigned user/department
- Key services running
- Configuration notes
- Change history
- Contact person

**Example VM Note for DC01:**

```
VM: DC01 - Primary Samba AD Domain Controller
Purpose: Samba Active Directory, DNS, Kerberos
Domain: smboffice.local
IP: 10.0.20.11
Services:
  - Samba AD DC (port 389, 636, 88, 464)
  - DNS (port 53)
  - Kerberos (port 88)
Installed: 2025-12-20
OS: Oracle Linux 9.3
Ansible Role: samba_ad_dc
Backup: Daily (critical)
Dependencies: Network, storage
Notes: Primary DC - DC02 is replica. Do not stop both simultaneously.
Owner: IT Administrator
Contact: admin@smboffice.local
Last Updated: 2025-12-24
```

**Adding Notes via GUI:**
1. Select VM
2. Click "Notes"
3. Enter documentation (supports basic formatting)

**Adding Notes via CLI:**
```bash
# Add note to DC01
qm set 101 --description "VM: DC01 - Primary Samba AD Domain Controller
Purpose: Samba Active Directory, DNS, Kerberos
Domain: smboffice.local
IP: 10.0.20.11
Services: Samba AD DC, DNS, Kerberos
OS: Oracle Linux 9.3
Backup: Daily (critical)
Owner: IT Administrator"
```

**Template for All VMs:**

Create a standardized note template:

```
VM: [VM_NAME] - [PURPOSE]
Function: [BUSINESS_FUNCTION]
Department: [DEPT]
IP: [IP_ADDRESS]
OS: [OS_VERSION]
Services: [LIST_SERVICES]
Ansible Role: [ROLE_NAME]
Backup Schedule: [DAILY/WEEKLY/MONTHLY]
Priority: [CRITICAL/HIGH/MEDIUM/LOW]
Dependencies: [OTHER_VMS]
Owner: [CONTACT_PERSON]
Notes: [SPECIAL_NOTES]
Created: [DATE]
Last Updated: [DATE]
```

### VM ID Numbering Convention

Logical VM ID ranges make organization intuitive:

**Our VM ID Strategy:**

| Range | Purpose | VMs |
|-------|---------|-----|
| 100-109 | Templates | 100: OL9 Template, 101: Ubuntu Template |
| 110-119 | Domain Controllers | 111: DC01, 112: DC02 |
| 120-129 | Infrastructure Services | 121: FILES01, 122: PRINT01 |
| 130-139 | Workstations - Admin/Exec | 131: ADMIN-WS01, 134: EXEC-WS01 |
| 140-149 | Workstations - HR | 142: HR-WS01 |
| 150-159 | Workstations - Finance | 153: FIN-WS01 |
| 160-169 | Workstations - Projects | 165: PROJ-WS01 |
| 170-179 | Workstations - Temporary | 176: INTERN-WS01 |
| 180-189 | Management/Monitoring | 181: MGMT01 |
| 900-999 | Test/Development VMs | Ad-hoc testing |

**Benefits:**
- Know VM type from ID (111 = domain controller)
- Room for expansion in each category
- Easy sorting in VM list
- Intuitive for scripting and automation

**Revised VM Assignments:**

| VM Name | New ID | Old ID | Category |
|---------|--------|--------|----------|
| OL9-Template | 100 | 100 | Template |
| Ubuntu-Template | 101 | - | Template |
| DC01 | 111 | 101 | Domain Controller |
| DC02 | 112 | 102 | Domain Controller |
| FILES01 | 121 | - | File Server |
| PRINT01 | 122 | - | Print Server |
| ADMIN-WS01 | 131 | - | Workstation (Admin) |
| EXEC-WS01 | 134 | - | Workstation (Exec) |
| HR-WS01 | 142 | - | Workstation (HR) |
| FIN-WS01 | 153 | - | Workstation (Finance) |
| PROJ-WS01 | 165 | - | Workstation (Projects) |
| INTERN-WS01 | 176 | - | Workstation (Temp) |
| MGMT01 | 181 | - | Management |

### HA Groups (High Availability)

**HA Groups** define VM priority and migration behavior (useful when you expand to cluster).

Even on single node, document HA intent:

**HA Priority Levels:**

| Priority | VMs | Behavior on Host Issues |
|----------|-----|------------------------|
| 1 (Highest) | DC01, DC02 | Try to recover first |
| 2 (High) | FILES01, MGMT01 | Recover after DCs |
| 3 (Medium) | PRINT01, FIN-WS01, EXEC-WS01 | Standard recovery |
| 4 (Low) | Workstations | Recover last, can tolerate downtime |
| 5 (Lowest) | INTERN-WS01 | Don't auto-recover |

**Creating HA Groups (Future Multi-Node Setup):**

```bash
# Create HA groups
ha-manager groupadd critical --nodes pve01 --restricted 0 --nofailback 0
ha-manager groupadd infrastructure --nodes pve01 --restricted 0 --nofailback 0
ha-manager groupadd workstations --nodes pve01 --restricted 0 --nofailback 1

# Add VMs to HA with priorities
ha-manager add vm:111 --group critical --max_relocate 1 --max_restart 3
ha-manager add vm:112 --group critical --max_relocate 1 --max_restart 3
ha-manager add vm:121 --group infrastructure --max_relocate 1 --max_restart 2
```

**For single-node:** Document in VM notes for future migration to cluster.

### Permission Groups by Department

**Use Case:** Allow department managers to access their workstations without seeing infrastructure.

**Permission Structure:**

```bash
# Create user groups
pveum groupadd hr-staff --comment "HR Department Staff"
pveum groupadd finance-staff --comment "Finance Department Staff"
pveum groupadd it-admins --comment "IT Administrators"

# Create users
pveum useradd hrmanager@pve --groups hr-staff
pveum useradd financemanager@pve --groups finance-staff
pveum useradd itadmin@pve --groups it-admins

# Assign pool permissions
pveum acl modify /pool/workstations-hr --groups hr-staff --roles PVEVMUser
pveum acl modify /pool/workstations-finance --groups finance-staff --roles PVEVMUser
pveum acl modify /pool/infrastructure --groups it-admins --roles PVEVMAdmin
pveum acl modify / --groups it-admins --roles Administrator
```

**Result:**
- HR manager logs in, sees only HR workstation pool
- Finance manager sees only Finance workstations
- IT admins see everything

### Backup Job Organization

Group VMs by backup schedule, not just pools:

**Backup Strategy:**

| Job Name | VMs | Schedule | Retention | Storage |
|----------|-----|----------|-----------|---------|
| backup-critical-daily | DC01, DC02, FILES01, MGMT01 | Daily 2:00 AM | 7 days | backup-nfs |
| backup-high-weekly | FIN-WS01, EXEC-WS01, PRINT01 | Weekly Sun 3:00 AM | 4 weeks | backup-nfs |
| backup-standard-weekly | HR-WS01, ADMIN-WS01, PROJ-WS01 | Weekly Sun 4:00 AM | 4 weeks | backup-nfs |
| backup-low-monthly | INTERN-WS01 | Monthly 1st Sun 5:00 AM | 3 months | backup-nfs |

**Creating Backup Jobs via GUI:**
1. Datacenter → Backup
2. Click "Add"
3. Select schedule, storage, retention
4. Select VMs or use "Pool" selection
5. Enable compression, notification

**Using Tags for Backup Selection:**

Instead of manually selecting VMs, use tags:
- Backup job filters by tag `backup-daily`
- Add new VM with tag → automatically included in backup

### Custom Properties and Metadata

**User-Defined Fields** (via API/CLI):

Store custom metadata in VM config:

```bash
# Add custom metadata
qm set 111 --meta "cost-center=IT-Operations"
qm set 111 --meta "compliance=SOX,HIPAA"
qm set 111 --meta "project=SMB-Infrastructure"
qm set 111 --meta "owner-email=admin@smboffice.local"
```

**Use Cases:**
- Cost allocation tracking
- Compliance tagging
- Project assignment
- Contact information
- License tracking

### Visual Organization Summary

**Complete Organization for DC01:**

```bash
# Create and configure DC01 with full organization
qm clone 100 111 --name DC01 --full --pool infrastructure

# Set resources
qm set 111 --cores 2 --memory 2048 --balloon 0

# Add tags
qm set 111 --tags "domain-controller,critical,backup-daily,oracle-linux-9,infrastructure,samba-ad"

# Add description/notes
qm set 111 --description "Primary Samba AD Domain Controller
IP: 10.0.20.11
Services: AD, DNS, Kerberos
Priority: CRITICAL
Backup: Daily
Owner: IT Admin"

# Set custom metadata
qm set 111 --meta "cost-center=IT-Operations"
qm set 111 --meta "compliance=SOX"
qm set 111 --meta "project=SMB-Infrastructure"
```

**GUI Result:**
- VM appears in "infrastructure" pool
- Shows tags: domain-controller, critical, backup-daily, oracle-linux-9
- Notes visible when selected
- Automatically included in daily backup job
- Sorted with other 110-119 domain controller VMs

### Organizational Checklist for Each VM

When creating any VM, complete this checklist:

- [ ] Assign to appropriate resource pool
- [ ] Add all relevant tags (function, dept, priority, OS, backup)
- [ ] Write comprehensive notes/description
- [ ] Use correct VM ID from numbering convention
- [ ] Set custom metadata (cost center, owner, compliance)
- [ ] Verify inclusion in correct backup job
- [ ] Document in Ansible inventory (for automation)

### Automation: Ansible Inventory Integration

All of this organization should be reflected in your Ansible inventory:

```yaml
# inventory/hosts.yml
all:
  children:
    domain_controllers:
      hosts:
        dc01:
          ansible_host: 10.0.20.11
          vm_id: 111
          proxmox_pool: infrastructure
          tags: ['domain-controller', 'critical', 'backup-daily']
          priority: critical
        dc02:
          ansible_host: 10.0.20.12
          vm_id: 112
          proxmox_pool: infrastructure
          tags: ['domain-controller', 'critical', 'backup-daily']
          priority: critical

    file_servers:
      hosts:
        files01:
          ansible_host: 10.0.20.21
          vm_id: 121
          proxmox_pool: infrastructure
          tags: ['file-server', 'high', 'backup-daily']
          priority: high

    workstations:
      children:
        hr_workstations:
          hosts:
            hr_ws01:
              ansible_host: 10.0.30.42
              vm_id: 142
              proxmox_pool: workstations-hr
              tags: ['workstation', 'dept-hr', 'medium']
              priority: medium

        finance_workstations:
          hosts:
            fin_ws01:
              ansible_host: 10.0.30.53
              vm_id: 153
              proxmox_pool: workstations-finance
              tags: ['workstation', 'dept-finance', 'high']
              priority: high
```

**Benefits:**
- Single source of truth (Ansible inventory)
- Ansible can create/update Proxmox organization
- Inventory matches Proxmox structure
- Easy to update and maintain

## Practical Example: Creating DC01

Let's walk through creating the first domain controller using best practices:

### Step 1: Clone from Template

```bash
# Clone Oracle Linux 9 template (VM 100) to create DC01 (VM 101)
qm clone 100 101 --name DC01 --full --storage local-lvm
```

### Step 2: Configure Resources

```bash
# Set CPU
qm set 101 --cores 2 --cpu host --cpuunits 2048

# Set RAM (2GB, no ballooning)
qm set 101 --memory 2048 --balloon 0

# Set disk options
qm set 101 --scsi0 local-lvm:vm-101-disk-0,cache=none,discard=on,iothread=1,ssd=1
```

### Step 3: Configure Network

```bash
# Add network interface on server VLAN
qm set 101 --net0 virtio,bridge=vmbr1,tag=20,firewall=0,queues=2
```

### Step 4: Set Boot Options

```bash
# Set boot order, enable QEMU guest agent
qm set 101 --boot order=scsi0 --agent enabled=1
```

### Step 5: Configure Cloud-Init (Optional but Recommended)

```bash
# Set IP configuration via cloud-init
qm set 101 --ipconfig0 ip=10.0.20.11/24,gw=10.0.20.1

# Set DNS servers
qm set 101 --nameserver "10.0.20.11 10.0.20.12"

# Set SSH key (replace with your public key)
qm set 101 --sshkey ~/.ssh/id_rsa.pub

# Set user
qm set 101 --ciuser administrator
```

### Step 6: Start VM

```bash
qm start 101
```

### Step 7: Verify and Connect

```bash
# Check status
qm status 101

# Get VNC access (for console)
qm terminal 101

# Or SSH (once cloud-init completes)
ssh administrator@10.0.20.11
```

**Repeat this process** for all 11 VMs, adjusting resources, networks, and IP addresses per the matrix above.

**Or better:** Use Ansible to automate all of this (upcoming article).

## Conclusion: Plan First, Deploy Confidently

Proper VM planning is the difference between a lab you're proud of and a frustrating mess you want to rebuild.

**Key Takeaways:**

1. **Size appropriately:** Not too much, not too little - monitor and adjust
2. **Segment networks:** Use VLANs/bridges for security and performance
3. **Use templates:** Build once, clone many
4. **Enable VirtIO:** Always, for network and disk
5. **Backup from day 1:** And test those backups regularly
6. **Document everything:** Your future self will thank you

With this foundation, you're ready to build a professional virtualization environment that performs well, uses resources efficiently, and provides the flexibility to experiment without risk.

**Next in this series:** We'll dive into Proxmox installation, initial configuration, and creating our VM templates from scratch.

---

## Quick Reference: VM Resource Matrix

| VM | vCPU | RAM | Disk | Network | Priority |
|----|------|-----|------|---------|----------|
| DC01 | 2 | 2GB | 32GB | vmbr1 (VLAN20) | CRITICAL |
| DC02 | 2 | 2GB | 32GB | vmbr1 (VLAN20) | CRITICAL |
| FILES01 | 4 | 4GB | 100GB | vmbr1 (VLAN20) | HIGH |
| PRINT01 | 2 | 2GB | 32GB | vmbr1 (VLAN20) | MEDIUM |
| ADMIN-WS01 | 2 | 2GB | 32GB | vmbr2 (VLAN30) | MEDIUM |
| HR-WS01 | 2 | 3GB | 32GB | vmbr2 (VLAN30) | MEDIUM |
| FIN-WS01 | 2 | 4GB | 32GB | vmbr2 (VLAN30) | HIGH |
| EXEC-WS01 | 4 | 4GB | 40GB | vmbr2 (VLAN30) | HIGH |
| PROJ-WS01 | 2 | 3GB | 32GB | vmbr2 (VLAN30) | MEDIUM |
| INTERN-WS01 | 2 | 2GB | 32GB | vmbr2 (VLAN30) | LOW |
| MGMT01 | 2 | 3GB | 40GB | vmbr0 (VLAN10) | HIGH |

**Total Resources:** 26 vCPU, 31GB RAM, 436GB disk

**Recommended Host:** 6-8 cores, 32-64GB RAM, 500GB-1TB SSD

---

## Additional Resources

**Proxmox Documentation:**
- [Official Proxmox VE Documentation](https://pve.proxmox.com/pve-docs/)
- [Proxmox VE Best Practices](https://pve.proxmox.com/wiki/Performance_Tweaks)

**Community Resources:**
- [r/Proxmox](https://reddit.com/r/Proxmox) - Active community
- [Proxmox Forums](https://forum.proxmox.com/) - Official support

**Our Series:**
- Article 01: Introduction to the SMB IT Blueprint
- Article 02: Proxmox VM Planning (this article)
- Article 03: Proxmox Installation and Initial Configuration (coming next)

---

**Author:** Richard Chamberlain
**Series:** SMB Office IT Blueprint
**Last Updated:** December 2025
**Contact:** [info@sebostechnology.com](mailto:info@sebostechnology.com)
